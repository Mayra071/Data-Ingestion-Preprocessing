ğŸ“Œ Overview

This repository provides utilities for data ingestion, preprocessing, and schema validation.
It supports multiple data sources (CSV, JSON, SQL) and standard preprocessing steps, with data versioning handled by DVC.

ğŸ¯ Goals

Build connectors for:

CSV files

JSON files

SQL databases (local samples)

Implement preprocessing:

Missing value imputation

Feature scaling

Encoding categorical features

Validate schemas with Pandera

Store raw and pre-processed datasets with DVC

ğŸ› ï¸ Tech Stack

Python 3.9+

pandas for data manipulation

scikit-learn for preprocessing

pandera for schema validation

SQLAlchemy / sqlite3 for database connectors

DVC for dataset version control

ğŸ“‚ Repository Structure
.
â”œâ”€â”€ artifacts/                # Raw & processed datasets (gitignored, tracked via DVC)
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ ingestion/            # Data ingestion connectors (CSV, JSON, SQL)
â”‚   â”œâ”€â”€ preprocessing/        # Preprocessing utilities
â”‚   â”œâ”€â”€ validation/           # Schema validation with Pandera
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ tests/                    # Unit tests
â”œâ”€â”€ dvc.yaml                  # DVC pipeline definitions
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ README.md                 # Project documentation
â””â”€â”€ .gitignore

ğŸ“… Day-wise Plan

Day 1: Define input sources (CSV, SQL). Implement Titanic CSV loader as baseline.

Day 2: Add preprocessing (missing values, scaling, encoding).

Day 3: Integrate schema validation with Pandera.

Day 4: Store raw + processed datasets using DVC.

Day 5: Demo ingestion pipeline + validation report.

ğŸš€ Getting Started
1. Clone the repository
git clone https://github.com/<your-username>/data-ingestion-preprocessing.git
cd data-ingestion-preprocessing

2. Create a virtual environment
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows

3. Install dependencies
pip install -r requirements.txt

4. Run pipeline
dvc repro

âœ… Example Usage
from src.ingestion.csv_loader import load_csv
from src.preprocessing.pipeline import preprocess_data
from src.validation.schema import validate_df

# Load dataset
df = load_csv("artifacts/train_data.csv")

# Preprocess
processed_df = preprocess_data(df)

# Validate
validate_df(processed_df)

ğŸ“Š Deliverables

Data ingestion + preprocessing module

Schema validation with Pandera

Preprocessed datasets versioned with DVC

Validation report + demo pipeline